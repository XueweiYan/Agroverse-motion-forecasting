{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76729f44",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf77ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e57e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"processed_training.npz\"\n",
    "train_data = np.load(train_path)\n",
    "#test_path = \"processed_testing.npz\"\n",
    "#test_data = np.load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9092c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim,\n",
    "                 hidden_dim,\n",
    "                 embed_dim,\n",
    "                ):\n",
    "        \n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTMCell(hidden_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, embedded):\n",
    "        lx = F.relu(self.linear(x))\n",
    "        embedded = self.lstm(lx, embedded)\n",
    "        return embedded\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_dim,\n",
    "                 hidden_dim,\n",
    "                 embed_dim,\n",
    "                ):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(out_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTMCell(hidden_dim, embed_dim)\n",
    "        self.linear2 = nn.Linear(embed_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lx = F.relu(self.linear1(x))\n",
    "        hidden = self.lstm(lx, hidden)\n",
    "        out = self.linear2(hidden[0])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9f2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "in_dim = 2\n",
    "out_dim = 2\n",
    "hidden_dim = 8\n",
    "embed_dim = 16\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.98\n",
    "num_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b41528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN, LSTM, 1dCNN, Transformer\n",
    "encoder = LSTMEncoder(in_dim = in_dim,\n",
    "               hidden_dim = hidden_dim,\n",
    "               embed_dim = embed_dim).to(device) # move model to gpu \n",
    "\n",
    "decoder = LSTMDecoder(out_dim = out_dim,\n",
    "               hidden_dim = hidden_dim,\n",
    "               embed_dim = embed_dim).to(device) # move model to gpu \n",
    "\n",
    "# Adaptive Moment Estimation computes adaptive learning rates for each parameter. \n",
    "# Compute the decaying averages of past and past squared gradients. \n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=1, gamma=decay_rate)\n",
    "decoder_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=1, gamma=decay_rate)\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea183644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, loss_function, roll_out):\n",
    "    train_mse = []\n",
    "    for i in range(370):\n",
    "        inp = torch.from_numpy(train_data[\"X\"][i * batch_size:(i+1) * batch_size]).float().to(device).reshape(batch_size, 19, 4)\n",
    "        inp = inp[:, :, :2]\n",
    "        tgt = torch.from_numpy(train_data[\"y\"][i * batch_size:(i+1) * batch_size]).float().to(device).reshape(batch_size, 30, 2)\n",
    "        tgt = tgt[:, :roll_out, :]\n",
    "        \n",
    "        embedded_vec = (\n",
    "            torch.zeros(batch_size, embed_dim).to(device),\n",
    "            torch.zeros(batch_size, embed_dim).to(device),\n",
    "        )\n",
    "        for step in range(19):\n",
    "            embedded_vec = encoder(inp[:, step, :], embedded_vec)\n",
    "            \n",
    "        pred = inp[:, -1, :]\n",
    "        loss = 0\n",
    "        for step in range(roll_out):\n",
    "            pred, hidden = decoder(pred, embedded_vec)\n",
    "            loss += loss_function(pred, tgt[:, step, :])\n",
    "        loss = loss / roll_out\n",
    "        \n",
    "        train_mse.append(loss.item()) \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    train_mse = round(np.sqrt(np.mean(train_mse)),5)\n",
    "    return train_mse\n",
    "\n",
    "def eval_epoch(encoder, decoder, loss_function, roll_out):\n",
    "    valid_mse = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(370,402):\n",
    "            inp = torch.from_numpy(train_data[\"X\"][i * batch_size:(i+1) * batch_size]).float().to(device).reshape(batch_size, 19, 4)\n",
    "            inp = inp[:, :, :2]\n",
    "            tgt = torch.from_numpy(train_data[\"y\"][i * batch_size:(i+1) * batch_size]).float().to(device).reshape(batch_size, 30, 2)\n",
    "            tgt = tgt[:, :roll_out, :]\n",
    "            \n",
    "            embedded_vec = (\n",
    "                torch.zeros(batch_size, embed_dim).to(device),\n",
    "                torch.zeros(batch_size, embed_dim).to(device),\n",
    "            )\n",
    "            for step in range(19):\n",
    "                embedded_vec = encoder(inp[:, step, :], embedded_vec)\n",
    "            \n",
    "            pred = inp[:, step, :]\n",
    "            loss = 0\n",
    "            for step in range(roll_out):\n",
    "                pred, hidden = decoder(pred, embedded_vec)\n",
    "                loss += loss_function(pred, tgt[:, step, :])\n",
    "            loss = loss / roll_out\n",
    "            valid_mse.append(loss.item())\n",
    "            \n",
    "        valid_mse = round(np.sqrt(np.mean(valid_mse)), 5)\n",
    "    return valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcffecc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 1.86 | Train RMSE: 0.09854 | Valid RMSE: 0.01193 | Current Roll Out: 1\n",
      "Epoch 2 | T: 1.86 | Train RMSE: 0.01169 | Valid RMSE: 0.01094 | Current Roll Out: 1\n",
      "Epoch 3 | T: 1.85 | Train RMSE: 0.01126 | Valid RMSE: 0.01069 | Current Roll Out: 1\n",
      "Epoch 4 | T: 1.85 | Train RMSE: 0.01090 | Valid RMSE: 0.01001 | Current Roll Out: 1\n",
      "Epoch 5 | T: 1.85 | Train RMSE: 0.01059 | Valid RMSE: 0.00929 | Current Roll Out: 1\n",
      "Epoch 6 | T: 1.85 | Train RMSE: 0.01004 | Valid RMSE: 0.00821 | Current Roll Out: 1\n",
      "Epoch 7 | T: 1.85 | Train RMSE: 0.00935 | Valid RMSE: 0.00867 | Current Roll Out: 1\n",
      "Epoch 8 | T: 1.85 | Train RMSE: 0.00921 | Valid RMSE: 0.00799 | Current Roll Out: 1\n",
      "Epoch 9 | T: 1.85 | Train RMSE: 0.00901 | Valid RMSE: 0.00782 | Current Roll Out: 1\n",
      "Epoch 10 | T: 1.85 | Train RMSE: 0.00890 | Valid RMSE: 0.00781 | Current Roll Out: 1\n",
      "Epoch 11 | T: 1.85 | Train RMSE: 0.00882 | Valid RMSE: 0.00786 | Current Roll Out: 1\n",
      "Epoch 12 | T: 1.85 | Train RMSE: 0.00878 | Valid RMSE: 0.00818 | Current Roll Out: 1\n",
      "Epoch 13 | T: 1.85 | Train RMSE: 0.00875 | Valid RMSE: 0.00803 | Current Roll Out: 1\n",
      "Epoch 14 | T: 1.85 | Train RMSE: 0.00872 | Valid RMSE: 0.00791 | Current Roll Out: 5\n",
      "Epoch 15 | T: 1.87 | Train RMSE: 0.06587 | Valid RMSE: 0.01950 | Current Roll Out: 5\n",
      "Epoch 16 | T: 1.86 | Train RMSE: 0.01977 | Valid RMSE: 0.01743 | Current Roll Out: 5\n",
      "Epoch 17 | T: 1.86 | Train RMSE: 0.01870 | Valid RMSE: 0.01660 | Current Roll Out: 5\n",
      "Epoch 18 | T: 1.86 | Train RMSE: 0.01796 | Valid RMSE: 0.01648 | Current Roll Out: 5\n",
      "Epoch 19 | T: 1.86 | Train RMSE: 0.01742 | Valid RMSE: 0.01680 | Current Roll Out: 5\n",
      "Epoch 20 | T: 1.86 | Train RMSE: 0.01700 | Valid RMSE: 0.01583 | Current Roll Out: 5\n",
      "Epoch 21 | T: 1.86 | Train RMSE: 0.01672 | Valid RMSE: 0.01602 | Current Roll Out: 5\n",
      "Epoch 22 | T: 1.87 | Train RMSE: 0.01643 | Valid RMSE: 0.01599 | Current Roll Out: 5\n",
      "Epoch 23 | T: 1.86 | Train RMSE: 0.01621 | Valid RMSE: 0.01591 | Current Roll Out: 5\n",
      "Epoch 24 | T: 1.87 | Train RMSE: 0.01599 | Valid RMSE: 0.01571 | Current Roll Out: 5\n",
      "Epoch 25 | T: 1.86 | Train RMSE: 0.01580 | Valid RMSE: 0.01540 | Current Roll Out: 5\n",
      "Epoch 26 | T: 1.86 | Train RMSE: 0.01566 | Valid RMSE: 0.01544 | Current Roll Out: 5\n",
      "Epoch 27 | T: 1.87 | Train RMSE: 0.01552 | Valid RMSE: 0.01510 | Current Roll Out: 5\n",
      "Epoch 28 | T: 1.86 | Train RMSE: 0.01539 | Valid RMSE: 0.01485 | Current Roll Out: 5\n",
      "Epoch 29 | T: 1.87 | Train RMSE: 0.01528 | Valid RMSE: 0.01466 | Current Roll Out: 5\n",
      "Epoch 30 | T: 1.86 | Train RMSE: 0.01517 | Valid RMSE: 0.01450 | Current Roll Out: 5\n",
      "Epoch 31 | T: 1.86 | Train RMSE: 0.01507 | Valid RMSE: 0.01430 | Current Roll Out: 5\n",
      "Epoch 32 | T: 1.86 | Train RMSE: 0.01498 | Valid RMSE: 0.01411 | Current Roll Out: 5\n",
      "Epoch 33 | T: 1.86 | Train RMSE: 0.01490 | Valid RMSE: 0.01401 | Current Roll Out: 5\n",
      "Epoch 34 | T: 1.86 | Train RMSE: 0.01484 | Valid RMSE: 0.01402 | Current Roll Out: 5\n",
      "Epoch 35 | T: 1.87 | Train RMSE: 0.01477 | Valid RMSE: 0.01444 | Current Roll Out: 5\n",
      "Epoch 36 | T: 1.86 | Train RMSE: 0.01470 | Valid RMSE: 0.01517 | Current Roll Out: 5\n",
      "Epoch 37 | T: 1.87 | Train RMSE: 0.01465 | Valid RMSE: 0.01492 | Current Roll Out: 10\n",
      "Epoch 38 | T: 1.89 | Train RMSE: 0.05670 | Valid RMSE: 0.03411 | Current Roll Out: 10\n",
      "Epoch 39 | T: 1.90 | Train RMSE: 0.03290 | Valid RMSE: 0.03017 | Current Roll Out: 10\n",
      "Epoch 40 | T: 1.89 | Train RMSE: 0.03024 | Valid RMSE: 0.02761 | Current Roll Out: 10\n",
      "Epoch 41 | T: 1.89 | Train RMSE: 0.02839 | Valid RMSE: 0.02518 | Current Roll Out: 10\n",
      "Epoch 42 | T: 1.88 | Train RMSE: 0.02729 | Valid RMSE: 0.02457 | Current Roll Out: 10\n",
      "Epoch 43 | T: 1.89 | Train RMSE: 0.02658 | Valid RMSE: 0.02442 | Current Roll Out: 10\n",
      "Epoch 44 | T: 1.89 | Train RMSE: 0.02609 | Valid RMSE: 0.02406 | Current Roll Out: 10\n",
      "Epoch 45 | T: 1.89 | Train RMSE: 0.02565 | Valid RMSE: 0.02393 | Current Roll Out: 10\n",
      "Epoch 46 | T: 1.89 | Train RMSE: 0.02531 | Valid RMSE: 0.02395 | Current Roll Out: 10\n",
      "Epoch 47 | T: 1.89 | Train RMSE: 0.02506 | Valid RMSE: 0.02378 | Current Roll Out: 10\n",
      "Epoch 48 | T: 1.88 | Train RMSE: 0.02479 | Valid RMSE: 0.02332 | Current Roll Out: 10\n",
      "Epoch 49 | T: 1.88 | Train RMSE: 0.02456 | Valid RMSE: 0.02305 | Current Roll Out: 10\n",
      "Epoch 50 | T: 1.88 | Train RMSE: 0.02437 | Valid RMSE: 0.02281 | Current Roll Out: 10\n",
      "Epoch 51 | T: 1.87 | Train RMSE: 0.02420 | Valid RMSE: 0.02283 | Current Roll Out: 10\n",
      "Epoch 52 | T: 1.87 | Train RMSE: 0.02407 | Valid RMSE: 0.02231 | Current Roll Out: 10\n",
      "Epoch 53 | T: 1.88 | Train RMSE: 0.02402 | Valid RMSE: 0.02186 | Current Roll Out: 10\n",
      "Epoch 54 | T: 1.88 | Train RMSE: 0.02381 | Valid RMSE: 0.02176 | Current Roll Out: 10\n",
      "Epoch 55 | T: 1.88 | Train RMSE: 0.02380 | Valid RMSE: 0.02198 | Current Roll Out: 10\n",
      "Epoch 56 | T: 1.88 | Train RMSE: 0.02356 | Valid RMSE: 0.02176 | Current Roll Out: 10\n",
      "Epoch 57 | T: 1.88 | Train RMSE: 0.02358 | Valid RMSE: 0.02207 | Current Roll Out: 10\n",
      "Epoch 58 | T: 1.90 | Train RMSE: 0.02341 | Valid RMSE: 0.02209 | Current Roll Out: 30\n",
      "Epoch 59 | T: 1.98 | Train RMSE: 0.14486 | Valid RMSE: 0.11928 | Current Roll Out: 30\n",
      "Epoch 60 | T: 1.97 | Train RMSE: 0.11566 | Valid RMSE: 0.11008 | Current Roll Out: 30\n",
      "Epoch 61 | T: 1.95 | Train RMSE: 0.10751 | Valid RMSE: 0.09978 | Current Roll Out: 30\n",
      "Epoch 62 | T: 1.95 | Train RMSE: 0.10121 | Valid RMSE: 0.09435 | Current Roll Out: 30\n",
      "Epoch 63 | T: 1.95 | Train RMSE: 0.09940 | Valid RMSE: 0.09819 | Current Roll Out: 30\n",
      "Epoch 64 | T: 1.95 | Train RMSE: 0.09640 | Valid RMSE: 0.09260 | Current Roll Out: 30\n",
      "Epoch 65 | T: 1.95 | Train RMSE: 0.09579 | Valid RMSE: 0.09218 | Current Roll Out: 30\n",
      "Epoch 66 | T: 1.95 | Train RMSE: 0.09455 | Valid RMSE: 0.09139 | Current Roll Out: 30\n",
      "Epoch 67 | T: 1.96 | Train RMSE: 0.09368 | Valid RMSE: 0.08994 | Current Roll Out: 30\n",
      "Epoch 68 | T: 1.96 | Train RMSE: 0.09249 | Valid RMSE: 0.09086 | Current Roll Out: 30\n",
      "Epoch 69 | T: 1.96 | Train RMSE: 0.09242 | Valid RMSE: 0.09237 | Current Roll Out: 30\n",
      "Epoch 70 | T: 1.95 | Train RMSE: 0.09180 | Valid RMSE: 0.08661 | Current Roll Out: 30\n",
      "Epoch 71 | T: 1.95 | Train RMSE: 0.09090 | Valid RMSE: 0.08816 | Current Roll Out: 30\n",
      "Epoch 72 | T: 1.95 | Train RMSE: 0.09052 | Valid RMSE: 0.08750 | Current Roll Out: 30\n",
      "Epoch 73 | T: 1.95 | Train RMSE: 0.09026 | Valid RMSE: 0.08591 | Current Roll Out: 30\n",
      "Epoch 74 | T: 1.95 | Train RMSE: 0.08980 | Valid RMSE: 0.08512 | Current Roll Out: 30\n",
      "Epoch 75 | T: 1.96 | Train RMSE: 0.08946 | Valid RMSE: 0.08398 | Current Roll Out: 30\n",
      "Epoch 76 | T: 1.96 | Train RMSE: 0.08910 | Valid RMSE: 0.08360 | Current Roll Out: 30\n",
      "Epoch 77 | T: 2.00 | Train RMSE: 0.08881 | Valid RMSE: 0.08327 | Current Roll Out: 30\n",
      "Epoch 78 | T: 1.95 | Train RMSE: 0.08854 | Valid RMSE: 0.08309 | Current Roll Out: 30\n",
      "Epoch 79 | T: 1.95 | Train RMSE: 0.08825 | Valid RMSE: 0.08298 | Current Roll Out: 30\n",
      "Epoch 80 | T: 1.95 | Train RMSE: 0.08799 | Valid RMSE: 0.08275 | Current Roll Out: 30\n",
      "Epoch 81 | T: 1.95 | Train RMSE: 0.08774 | Valid RMSE: 0.08252 | Current Roll Out: 30\n",
      "Epoch 82 | T: 1.95 | Train RMSE: 0.08750 | Valid RMSE: 0.08239 | Current Roll Out: 30\n",
      "Epoch 83 | T: 1.99 | Train RMSE: 0.08724 | Valid RMSE: 0.08231 | Current Roll Out: 30\n",
      "Epoch 84 | T: 1.95 | Train RMSE: 0.08703 | Valid RMSE: 0.08201 | Current Roll Out: 30\n",
      "Epoch 85 | T: 1.96 | Train RMSE: 0.08685 | Valid RMSE: 0.08241 | Current Roll Out: 30\n",
      "Epoch 86 | T: 1.96 | Train RMSE: 0.08666 | Valid RMSE: 0.08390 | Current Roll Out: 30\n",
      "Epoch 87 | T: 1.96 | Train RMSE: 0.08654 | Valid RMSE: 0.08282 | Current Roll Out: 30\n"
     ]
    }
   ],
   "source": [
    "train_rmse = []\n",
    "valid_rmse = []\n",
    "min_rmse = 10e8\n",
    "worse_count = 0\n",
    "\n",
    "roll_outs = [1, 5, 10, 30]\n",
    "roll_out_id = 0\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    start = time.time()\n",
    "\n",
    "    # model.train() # if you use dropout or batchnorm. \n",
    "    train_rmse.append(train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, loss_fun, roll_outs[roll_out_id]))\n",
    "\n",
    "    # model.eval()\n",
    "    val_rmse = eval_epoch(encoder, decoder, loss_fun, roll_outs[roll_out_id])\n",
    "    valid_rmse.append(val_rmse)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_rmse[-1] < min_rmse:\n",
    "        min_rmse = valid_rmse[-1]\n",
    "        best_model = (encoder, decoder)\n",
    "        worse_count = 0\n",
    "        # torch.save([best_model, i, get_lr(optimizer)], name + \".pth\")\n",
    "    else:\n",
    "        worse_count += 1\n",
    "    \n",
    "    if worse_count > 3:\n",
    "        roll_out_id += 1\n",
    "        worse_count = 0\n",
    "        min_rmse = 10e8\n",
    "    \n",
    "    if roll_out_id >= len(roll_outs):\n",
    "        break\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # Early Stopping\n",
    "    # if (len(train_rmse) > 100 and np.mean(valid_rmse[-5:]) >= np.mean(valid_rmse[-10:-5])):\n",
    "    #        break\n",
    "\n",
    "    # Learning Rate Decay        \n",
    "    encoder_scheduler.step()\n",
    "    decoder_scheduler.step()\n",
    "    \n",
    "    print(\"Epoch {} | T: {:0.2f} | Train RMSE: {:0.5f} | Valid RMSE: {:0.5f} | Current Roll Out: {}\".format(i + 1, (end-start) / 60, train_rmse[-1], valid_rmse[-1], roll_outs[roll_out_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d022f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./val_in/\"\n",
    "test_pkl_list = glob(os.path.join(test_path, '*'))\n",
    "test_pkl_list.sort()\n",
    "\n",
    "test_preds = []\n",
    "for idx in range(len(test_pkl_list)):\n",
    "    with open(test_pkl_list[idx], 'rb') as f:\n",
    "        test_sample = pickle.load(f)\n",
    "        pred_id = np.where(test_sample[\"track_id\"] == test_sample['agent_id'])[0][0]\n",
    "        car_mask = np.where(test_sample[\"car_mask\"] == 1)[0]\n",
    "        inp_scene = test_sample['p_in'][car_mask]\n",
    "\n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        \n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        inp = torch.from_numpy(inp).float().to(device).unsqueeze(0)\n",
    "        \n",
    "        encoder, decoder = best_model\n",
    "        embedded_vec = (\n",
    "            torch.zeros(1, embed_dim).to(device),\n",
    "            torch.zeros(1, embed_dim).to(device),\n",
    "        )\n",
    "        for step in range(19):\n",
    "            embedded_vec = encoder(inp[:, step, :], embedded_vec)\n",
    "        \n",
    "        preds = []\n",
    "        pred = inp[:, step, :]\n",
    "        for step in range(30):\n",
    "            pred, hidden = decoder(pred, embedded_vec)\n",
    "            preds.append(pred.cpu().data.numpy())\n",
    "        \n",
    "        preds = np.array(preds).reshape(30, 2)\n",
    "        \n",
    "        # De-Normalization !\n",
    "        preds = preds * (max_vecs - min_vecs) +  min_vecs\n",
    "        test_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c127c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1).astype(int)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df.to_csv('test_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88637f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
