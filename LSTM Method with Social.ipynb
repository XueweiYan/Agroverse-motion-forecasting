{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76729f44",
   "metadata": {},
   "source": [
    "# LSTM with social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf77ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e57e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"processed_train.npz\"\n",
    "train_data = np.load(train_path)\n",
    "test_path = \"processed_val_in.npz\"\n",
    "test_data = np.load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9092c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim,\n",
    "                 hidden_dim,\n",
    "                 embed_dim,\n",
    "                ):\n",
    "        \n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTMCell(hidden_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, embedded):\n",
    "        lx = F.relu(self.linear(x))\n",
    "        embedded = self.lstm(lx, embedded)\n",
    "        return embedded\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_dim,\n",
    "                 hidden_dim,\n",
    "                 embed_dim,\n",
    "                ):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(out_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTMCell(hidden_dim, embed_dim)\n",
    "        self.linear2 = nn.Linear(embed_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lx = F.relu(self.linear1(x))\n",
    "        hidden = self.lstm(lx, hidden)\n",
    "        out = self.linear2(hidden[0])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9f2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "in_dim = 5\n",
    "out_dim = 2\n",
    "hidden_dim = 8\n",
    "embed_dim = 16\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.99\n",
    "num_epoch = 1000\n",
    "roll_outs = [1, 3, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b41528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN, LSTM, 1dCNN, Transformer\n",
    "encoder = LSTMEncoder(in_dim = in_dim,\n",
    "               hidden_dim = hidden_dim,\n",
    "               embed_dim = embed_dim).to(device) # move model to gpu \n",
    "\n",
    "decoder = LSTMDecoder(out_dim = out_dim,\n",
    "               hidden_dim = hidden_dim,\n",
    "               embed_dim = embed_dim).to(device) # move model to gpu \n",
    "\n",
    "# Adaptive Moment Estimation computes adaptive learning rates for each parameter. \n",
    "# Compute the decaying averages of past and past squared gradients. \n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=1, gamma=decay_rate)\n",
    "decoder_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=1, gamma=decay_rate)\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea183644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, loss_function, roll_out):\n",
    "    train_mse = []\n",
    "    shuffler = np.random.permutation(np.arange(390 * batch_size))\n",
    "    for i in range(390):\n",
    "        batch_ids = shuffler[i * batch_size:(i+1) * batch_size]\n",
    "        inp = torch.from_numpy(train_data[\"X\"][batch_ids]).float().to(device).reshape(batch_size, 19, -1)\n",
    "        tgt = torch.from_numpy(train_data[\"y\"][batch_ids]).float().to(device).reshape(batch_size, 30, 2)\n",
    "        tgt = tgt[:, :roll_out, :]\n",
    "\n",
    "        embedded_vec = (\n",
    "            torch.zeros(batch_size, embed_dim).to(device),\n",
    "            torch.zeros(batch_size, embed_dim).to(device),\n",
    "        )\n",
    "        for step in range(19):\n",
    "            embedded_vec = encoder(inp[:, step, :], embedded_vec)\n",
    "            \n",
    "        pred = inp[:, -1, :2]\n",
    "        loss = 0\n",
    "        for step in range(roll_out):\n",
    "            pred, embedded_vec = decoder(pred, embedded_vec)\n",
    "            loss += loss_function(pred, tgt[:, step, :])\n",
    "        loss = loss / roll_out\n",
    "        \n",
    "        train_mse.append(loss.item()) \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    train_mse = round(np.sqrt(np.mean(train_mse)),5)\n",
    "    return train_mse\n",
    "\n",
    "def eval_epoch(encoder, decoder, loss_function, roll_out):\n",
    "    valid_mse = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(390, 402):\n",
    "            inp = torch.from_numpy(train_data[\"X\"][i * batch_size:(i+1) * batch_size]).float().to(device).reshape(batch_size, 19, -1)\n",
    "            tgt = torch.from_numpy(train_data[\"y\"][i * batch_size:(i+1) * batch_size]).float().to(device).reshape(batch_size, 30, 2)\n",
    "            tgt = tgt[:, :roll_out, :]\n",
    "            \n",
    "            embedded_vec = (\n",
    "                torch.zeros(batch_size, embed_dim).to(device),\n",
    "                torch.zeros(batch_size, embed_dim).to(device),\n",
    "            )\n",
    "            for step in range(19):\n",
    "                embedded_vec = encoder(inp[:, step, :], embedded_vec)\n",
    "            \n",
    "            pred = inp[:, step, :2]\n",
    "            loss = 0\n",
    "            for step in range(roll_out):\n",
    "                pred, embedded_vec = decoder(pred, embedded_vec)\n",
    "                loss += loss_function(pred, tgt[:, step, :])\n",
    "            loss = loss / roll_out\n",
    "            valid_mse.append(loss.item())\n",
    "            \n",
    "        valid_mse = round(np.sqrt(np.mean(valid_mse)), 5)\n",
    "    return valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcffecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 2.12 | Train RMSE: 5.02455 | Valid RMSE: 1.50566 | Current Roll Out: 1\n",
      "Epoch 2 | T: 2.11 | Train RMSE: 1.31761 | Valid RMSE: 0.90033 | Current Roll Out: 1\n",
      "Epoch 3 | T: 2.11 | Train RMSE: 0.93927 | Valid RMSE: 0.67411 | Current Roll Out: 1\n",
      "Epoch 4 | T: 2.11 | Train RMSE: 0.75489 | Valid RMSE: 0.54766 | Current Roll Out: 1\n",
      "Epoch 5 | T: 2.12 | Train RMSE: 0.63777 | Valid RMSE: 0.46529 | Current Roll Out: 1\n",
      "Epoch 6 | T: 2.11 | Train RMSE: 0.55683 | Valid RMSE: 0.40940 | Current Roll Out: 1\n",
      "Epoch 7 | T: 2.11 | Train RMSE: 0.49988 | Valid RMSE: 0.37164 | Current Roll Out: 1\n",
      "Epoch 8 | T: 2.11 | Train RMSE: 0.45417 | Valid RMSE: 0.35803 | Current Roll Out: 1\n",
      "Epoch 9 | T: 2.11 | Train RMSE: 0.42348 | Valid RMSE: 0.32756 | Current Roll Out: 1\n",
      "Epoch 10 | T: 2.11 | Train RMSE: 0.39868 | Valid RMSE: 0.31626 | Current Roll Out: 1\n",
      "Epoch 11 | T: 2.11 | Train RMSE: 0.37998 | Valid RMSE: 0.31515 | Current Roll Out: 1\n",
      "Epoch 12 | T: 2.11 | Train RMSE: 0.36338 | Valid RMSE: 0.32287 | Current Roll Out: 1\n",
      "Epoch 13 | T: 2.11 | Train RMSE: 0.35231 | Valid RMSE: 0.31811 | Current Roll Out: 1\n",
      "Epoch 14 | T: 2.12 | Train RMSE: 0.34457 | Valid RMSE: 0.31588 | Current Roll Out: 1\n",
      "Epoch 15 | T: 2.12 | Train RMSE: 0.33770 | Valid RMSE: 0.32248 | Current Roll Out: 1\n",
      "Epoch 16 | T: 2.13 | Train RMSE: 0.33337 | Valid RMSE: 0.30660 | Current Roll Out: 1\n",
      "Epoch 17 | T: 2.12 | Train RMSE: 0.32623 | Valid RMSE: 0.31355 | Current Roll Out: 1\n",
      "Epoch 18 | T: 2.12 | Train RMSE: 0.32599 | Valid RMSE: 0.31141 | Current Roll Out: 1\n",
      "Epoch 19 | T: 2.12 | Train RMSE: 0.32310 | Valid RMSE: 0.31089 | Current Roll Out: 1\n",
      "Epoch 20 | T: 2.16 | Train RMSE: 0.33288 | Valid RMSE: 0.32956 | Current Roll Out: 1\n",
      "Epoch 21 | T: 2.11 | Train RMSE: 0.31585 | Valid RMSE: 0.31260 | Current Roll Out: 1\n",
      "Epoch 22 | T: 2.12 | Train RMSE: 0.31410 | Valid RMSE: 0.31717 | Current Roll Out: 1\n",
      "Epoch 23 | T: 2.12 | Train RMSE: 0.31623 | Valid RMSE: 0.32077 | Current Roll Out: 1\n",
      "Epoch 24 | T: 2.12 | Train RMSE: 0.31187 | Valid RMSE: 0.31192 | Current Roll Out: 1\n",
      "Epoch 25 | T: 2.12 | Train RMSE: 0.31159 | Valid RMSE: 0.33599 | Current Roll Out: 1\n",
      "Epoch 26 | T: 2.12 | Train RMSE: 0.31642 | Valid RMSE: 0.30772 | Current Roll Out: 1\n",
      "Epoch 27 | T: 2.12 | Train RMSE: 0.31314 | Valid RMSE: 0.32799 | Current Roll Out: 3\n",
      "Epoch 28 | T: 2.13 | Train RMSE: 1.34495 | Valid RMSE: 0.70896 | Current Roll Out: 3\n",
      "Epoch 29 | T: 2.13 | Train RMSE: 0.73831 | Valid RMSE: 0.58412 | Current Roll Out: 3\n",
      "Epoch 30 | T: 2.13 | Train RMSE: 0.62172 | Valid RMSE: 0.51280 | Current Roll Out: 3\n",
      "Epoch 31 | T: 2.13 | Train RMSE: 0.55986 | Valid RMSE: 0.46339 | Current Roll Out: 3\n",
      "Epoch 32 | T: 2.13 | Train RMSE: 0.52280 | Valid RMSE: 0.44676 | Current Roll Out: 3\n",
      "Epoch 33 | T: 2.13 | Train RMSE: 0.49645 | Valid RMSE: 0.45663 | Current Roll Out: 3\n",
      "Epoch 34 | T: 2.13 | Train RMSE: 0.47704 | Valid RMSE: 0.42567 | Current Roll Out: 3\n",
      "Epoch 35 | T: 2.13 | Train RMSE: 0.46823 | Valid RMSE: 0.44165 | Current Roll Out: 3\n",
      "Epoch 36 | T: 2.13 | Train RMSE: 0.45765 | Valid RMSE: 0.42440 | Current Roll Out: 3\n",
      "Epoch 37 | T: 2.13 | Train RMSE: 0.45853 | Valid RMSE: 0.41656 | Current Roll Out: 3\n",
      "Epoch 38 | T: 2.13 | Train RMSE: 0.44434 | Valid RMSE: 0.52353 | Current Roll Out: 3\n",
      "Epoch 39 | T: 2.13 | Train RMSE: 0.44097 | Valid RMSE: 0.41698 | Current Roll Out: 3\n",
      "Epoch 40 | T: 2.13 | Train RMSE: 0.43955 | Valid RMSE: 0.44049 | Current Roll Out: 3\n",
      "Epoch 41 | T: 2.13 | Train RMSE: 0.42756 | Valid RMSE: 0.41318 | Current Roll Out: 3\n",
      "Epoch 42 | T: 2.13 | Train RMSE: 0.43745 | Valid RMSE: 0.46075 | Current Roll Out: 3\n",
      "Epoch 43 | T: 2.13 | Train RMSE: 0.42928 | Valid RMSE: 0.41737 | Current Roll Out: 3\n",
      "Epoch 44 | T: 2.13 | Train RMSE: 0.42553 | Valid RMSE: 0.41888 | Current Roll Out: 3\n",
      "Epoch 45 | T: 2.13 | Train RMSE: 0.42726 | Valid RMSE: 0.44491 | Current Roll Out: 3\n",
      "Epoch 46 | T: 2.13 | Train RMSE: 0.42248 | Valid RMSE: 0.46757 | Current Roll Out: 3\n",
      "Epoch 47 | T: 2.13 | Train RMSE: 0.42462 | Valid RMSE: 0.41530 | Current Roll Out: 3\n",
      "Epoch 48 | T: 2.13 | Train RMSE: 0.41996 | Valid RMSE: 0.41333 | Current Roll Out: 3\n",
      "Epoch 49 | T: 2.13 | Train RMSE: 0.41636 | Valid RMSE: 0.43769 | Current Roll Out: 3\n",
      "Epoch 50 | T: 2.13 | Train RMSE: 0.42447 | Valid RMSE: 0.41093 | Current Roll Out: 3\n",
      "Epoch 51 | T: 2.12 | Train RMSE: 0.41672 | Valid RMSE: 0.42233 | Current Roll Out: 3\n",
      "Epoch 52 | T: 2.12 | Train RMSE: 0.42011 | Valid RMSE: 0.41852 | Current Roll Out: 3\n",
      "Epoch 53 | T: 2.11 | Train RMSE: 0.42272 | Valid RMSE: 0.44288 | Current Roll Out: 3\n",
      "Epoch 54 | T: 2.11 | Train RMSE: 0.41636 | Valid RMSE: 0.40937 | Current Roll Out: 3\n",
      "Epoch 55 | T: 2.11 | Train RMSE: 0.41391 | Valid RMSE: 0.41016 | Current Roll Out: 3\n",
      "Epoch 56 | T: 2.11 | Train RMSE: 0.41395 | Valid RMSE: 0.41711 | Current Roll Out: 3\n",
      "Epoch 57 | T: 2.11 | Train RMSE: 0.41457 | Valid RMSE: 0.40759 | Current Roll Out: 3\n",
      "Epoch 58 | T: 2.11 | Train RMSE: 0.41768 | Valid RMSE: 0.41938 | Current Roll Out: 3\n",
      "Epoch 59 | T: 2.11 | Train RMSE: 0.41204 | Valid RMSE: 0.42153 | Current Roll Out: 3\n",
      "Epoch 60 | T: 2.11 | Train RMSE: 0.41535 | Valid RMSE: 0.40518 | Current Roll Out: 3\n",
      "Epoch 61 | T: 2.11 | Train RMSE: 0.41453 | Valid RMSE: 0.41345 | Current Roll Out: 3\n",
      "Epoch 62 | T: 2.12 | Train RMSE: 0.41383 | Valid RMSE: 0.41198 | Current Roll Out: 3\n",
      "Epoch 63 | T: 2.11 | Train RMSE: 0.41252 | Valid RMSE: 0.40715 | Current Roll Out: 3\n",
      "Epoch 64 | T: 2.11 | Train RMSE: 0.40974 | Valid RMSE: 0.41535 | Current Roll Out: 3\n",
      "Epoch 65 | T: 2.11 | Train RMSE: 0.41529 | Valid RMSE: 0.41683 | Current Roll Out: 3\n",
      "Epoch 66 | T: 2.12 | Train RMSE: 0.41010 | Valid RMSE: 0.40981 | Current Roll Out: 3\n",
      "Epoch 67 | T: 2.11 | Train RMSE: 0.40866 | Valid RMSE: 0.41040 | Current Roll Out: 3\n",
      "Epoch 68 | T: 2.12 | Train RMSE: 0.40894 | Valid RMSE: 0.41153 | Current Roll Out: 3\n",
      "Epoch 69 | T: 2.12 | Train RMSE: 0.40864 | Valid RMSE: 0.41387 | Current Roll Out: 3\n",
      "Epoch 70 | T: 2.12 | Train RMSE: 0.40972 | Valid RMSE: 0.40017 | Current Roll Out: 3\n",
      "Epoch 71 | T: 2.13 | Train RMSE: 0.41163 | Valid RMSE: 0.41834 | Current Roll Out: 3\n",
      "Epoch 72 | T: 2.13 | Train RMSE: 0.40853 | Valid RMSE: 0.42353 | Current Roll Out: 3\n",
      "Epoch 73 | T: 2.12 | Train RMSE: 0.40588 | Valid RMSE: 0.40123 | Current Roll Out: 3\n",
      "Epoch 74 | T: 2.13 | Train RMSE: 0.40666 | Valid RMSE: 0.41056 | Current Roll Out: 3\n",
      "Epoch 75 | T: 2.11 | Train RMSE: 0.40779 | Valid RMSE: 0.40383 | Current Roll Out: 3\n",
      "Epoch 76 | T: 2.11 | Train RMSE: 0.40546 | Valid RMSE: 0.41991 | Current Roll Out: 3\n",
      "Epoch 77 | T: 2.11 | Train RMSE: 0.40567 | Valid RMSE: 0.39974 | Current Roll Out: 3\n",
      "Epoch 78 | T: 2.11 | Train RMSE: 0.40326 | Valid RMSE: 0.40793 | Current Roll Out: 3\n",
      "Epoch 79 | T: 2.11 | Train RMSE: 0.40411 | Valid RMSE: 0.44084 | Current Roll Out: 3\n",
      "Epoch 80 | T: 2.11 | Train RMSE: 0.40200 | Valid RMSE: 0.40429 | Current Roll Out: 3\n",
      "Epoch 81 | T: 2.11 | Train RMSE: 0.40330 | Valid RMSE: 0.41148 | Current Roll Out: 3\n",
      "Epoch 82 | T: 2.11 | Train RMSE: 0.40418 | Valid RMSE: 0.39537 | Current Roll Out: 3\n",
      "Epoch 83 | T: 2.11 | Train RMSE: 0.40322 | Valid RMSE: 0.39794 | Current Roll Out: 3\n",
      "Epoch 84 | T: 2.11 | Train RMSE: 0.40055 | Valid RMSE: 0.40161 | Current Roll Out: 3\n",
      "Epoch 85 | T: 2.11 | Train RMSE: 0.40122 | Valid RMSE: 0.40723 | Current Roll Out: 3\n",
      "Epoch 86 | T: 2.11 | Train RMSE: 0.39940 | Valid RMSE: 0.40253 | Current Roll Out: 3\n",
      "Epoch 87 | T: 2.11 | Train RMSE: 0.40013 | Valid RMSE: 0.39864 | Current Roll Out: 3\n",
      "Epoch 88 | T: 2.12 | Train RMSE: 0.40034 | Valid RMSE: 0.39623 | Current Roll Out: 3\n",
      "Epoch 89 | T: 2.12 | Train RMSE: 0.39808 | Valid RMSE: 0.40468 | Current Roll Out: 3\n",
      "Epoch 90 | T: 2.12 | Train RMSE: 0.39844 | Valid RMSE: 0.39767 | Current Roll Out: 3\n",
      "Epoch 91 | T: 2.12 | Train RMSE: 0.40096 | Valid RMSE: 0.40237 | Current Roll Out: 3\n",
      "Epoch 92 | T: 2.11 | Train RMSE: 0.39628 | Valid RMSE: 0.40027 | Current Roll Out: 3\n",
      "Epoch 93 | T: 2.12 | Train RMSE: 0.39657 | Valid RMSE: 0.39587 | Current Roll Out: 10\n",
      "Epoch 94 | T: 2.16 | Train RMSE: 1.58941 | Valid RMSE: 1.14288 | Current Roll Out: 10\n",
      "Epoch 95 | T: 2.17 | Train RMSE: 1.20167 | Valid RMSE: 1.11036 | Current Roll Out: 10\n",
      "Epoch 96 | T: 2.17 | Train RMSE: 1.14014 | Valid RMSE: 1.03056 | Current Roll Out: 10\n",
      "Epoch 97 | T: 2.16 | Train RMSE: 1.06036 | Valid RMSE: 0.93052 | Current Roll Out: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 | T: 2.16 | Train RMSE: 0.89774 | Valid RMSE: 0.87257 | Current Roll Out: 10\n",
      "Epoch 99 | T: 2.16 | Train RMSE: 0.87920 | Valid RMSE: 0.87240 | Current Roll Out: 10\n",
      "Epoch 100 | T: 2.16 | Train RMSE: 0.87843 | Valid RMSE: 0.86563 | Current Roll Out: 10\n",
      "Epoch 101 | T: 2.16 | Train RMSE: 0.87222 | Valid RMSE: 0.86872 | Current Roll Out: 10\n",
      "Epoch 102 | T: 2.16 | Train RMSE: 0.87066 | Valid RMSE: 0.87086 | Current Roll Out: 10\n",
      "Epoch 103 | T: 2.16 | Train RMSE: 0.86694 | Valid RMSE: 0.86109 | Current Roll Out: 10\n",
      "Epoch 104 | T: 2.16 | Train RMSE: 0.86487 | Valid RMSE: 0.87894 | Current Roll Out: 10\n",
      "Epoch 105 | T: 2.17 | Train RMSE: 0.86365 | Valid RMSE: 0.87010 | Current Roll Out: 10\n",
      "Epoch 106 | T: 2.16 | Train RMSE: 0.86446 | Valid RMSE: 0.88269 | Current Roll Out: 10\n",
      "Epoch 107 | T: 2.16 | Train RMSE: 0.86021 | Valid RMSE: 0.89646 | Current Roll Out: 10\n",
      "Epoch 108 | T: 2.16 | Train RMSE: 0.85115 | Valid RMSE: 0.86604 | Current Roll Out: 10\n",
      "Epoch 109 | T: 2.16 | Train RMSE: 0.85960 | Valid RMSE: 0.85487 | Current Roll Out: 10\n",
      "Epoch 110 | T: 2.16 | Train RMSE: 0.85139 | Valid RMSE: 0.86144 | Current Roll Out: 10\n",
      "Epoch 111 | T: 2.16 | Train RMSE: 0.85375 | Valid RMSE: 0.88387 | Current Roll Out: 10\n",
      "Epoch 112 | T: 2.16 | Train RMSE: 0.85196 | Valid RMSE: 0.86463 | Current Roll Out: 10\n",
      "Epoch 113 | T: 2.16 | Train RMSE: 0.85448 | Valid RMSE: 0.86560 | Current Roll Out: 10\n",
      "Epoch 114 | T: 2.16 | Train RMSE: 0.84758 | Valid RMSE: 0.87532 | Current Roll Out: 10\n",
      "Epoch 115 | T: 2.16 | Train RMSE: 0.84964 | Valid RMSE: 0.86815 | Current Roll Out: 10\n",
      "Epoch 116 | T: 2.16 | Train RMSE: 0.85122 | Valid RMSE: 0.86071 | Current Roll Out: 10\n",
      "Epoch 117 | T: 2.17 | Train RMSE: 0.84867 | Valid RMSE: 0.88114 | Current Roll Out: 10\n",
      "Epoch 118 | T: 2.16 | Train RMSE: 0.84622 | Valid RMSE: 0.85626 | Current Roll Out: 10\n",
      "Epoch 119 | T: 2.16 | Train RMSE: 0.84455 | Valid RMSE: 0.87400 | Current Roll Out: 10\n",
      "Epoch 120 | T: 2.17 | Train RMSE: 0.85222 | Valid RMSE: 0.85601 | Current Roll Out: 30\n",
      "Epoch 121 | T: 2.21 | Train RMSE: 4.12204 | Valid RMSE: 3.53085 | Current Roll Out: 30\n",
      "Epoch 122 | T: 2.20 | Train RMSE: 3.52380 | Valid RMSE: 3.31243 | Current Roll Out: 30\n",
      "Epoch 123 | T: 2.21 | Train RMSE: 3.34725 | Valid RMSE: 3.22780 | Current Roll Out: 30\n",
      "Epoch 124 | T: 2.21 | Train RMSE: 3.26210 | Valid RMSE: 3.14735 | Current Roll Out: 30\n",
      "Epoch 125 | T: 2.22 | Train RMSE: 3.22889 | Valid RMSE: 3.16255 | Current Roll Out: 30\n",
      "Epoch 126 | T: 2.21 | Train RMSE: 3.21708 | Valid RMSE: 3.11802 | Current Roll Out: 30\n",
      "Epoch 127 | T: 2.21 | Train RMSE: 3.19118 | Valid RMSE: 3.09628 | Current Roll Out: 30\n",
      "Epoch 128 | T: 2.24 | Train RMSE: 3.16922 | Valid RMSE: 3.07678 | Current Roll Out: 30\n",
      "Epoch 129 | T: 2.20 | Train RMSE: 3.13692 | Valid RMSE: 3.05257 | Current Roll Out: 30\n",
      "Epoch 130 | T: 2.16 | Train RMSE: 3.11182 | Valid RMSE: 3.04152 | Current Roll Out: 30\n",
      "Epoch 131 | T: 2.19 | Train RMSE: 3.09563 | Valid RMSE: 3.02870 | Current Roll Out: 30\n",
      "Epoch 132 | T: 2.18 | Train RMSE: 3.09217 | Valid RMSE: 3.03996 | Current Roll Out: 30\n",
      "Epoch 133 | T: 2.19 | Train RMSE: 3.08769 | Valid RMSE: 3.01406 | Current Roll Out: 30\n",
      "Epoch 134 | T: 2.19 | Train RMSE: 3.07698 | Valid RMSE: 3.00517 | Current Roll Out: 30\n",
      "Epoch 135 | T: 2.18 | Train RMSE: 3.06847 | Valid RMSE: 3.07668 | Current Roll Out: 30\n",
      "Epoch 136 | T: 2.19 | Train RMSE: 3.07378 | Valid RMSE: 3.01084 | Current Roll Out: 30\n",
      "Epoch 137 | T: 2.20 | Train RMSE: 3.06053 | Valid RMSE: 2.99432 | Current Roll Out: 30\n",
      "Epoch 138 | T: 2.19 | Train RMSE: 3.06024 | Valid RMSE: 3.10178 | Current Roll Out: 30\n",
      "Epoch 139 | T: 2.19 | Train RMSE: 3.05555 | Valid RMSE: 3.00185 | Current Roll Out: 30\n",
      "Epoch 140 | T: 2.20 | Train RMSE: 3.06290 | Valid RMSE: 2.99507 | Current Roll Out: 30\n",
      "Epoch 141 | T: 2.20 | Train RMSE: 3.05114 | Valid RMSE: 3.07083 | Current Roll Out: 30\n",
      "Epoch 142 | T: 2.20 | Train RMSE: 3.06674 | Valid RMSE: 3.02733 | Current Roll Out: 30\n",
      "Epoch 143 | T: 2.19 | Train RMSE: 3.06055 | Valid RMSE: 2.99917 | Current Roll Out: 30\n",
      "Epoch 144 | T: 2.20 | Train RMSE: 3.06659 | Valid RMSE: 2.99750 | Current Roll Out: 30\n",
      "Epoch 145 | T: 2.20 | Train RMSE: 3.05036 | Valid RMSE: 2.99208 | Current Roll Out: 30\n",
      "Epoch 146 | T: 2.20 | Train RMSE: 3.05841 | Valid RMSE: 3.02340 | Current Roll Out: 30\n",
      "Epoch 147 | T: 2.21 | Train RMSE: 3.04825 | Valid RMSE: 2.99513 | Current Roll Out: 30\n",
      "Epoch 148 | T: 2.19 | Train RMSE: 3.04227 | Valid RMSE: 2.99804 | Current Roll Out: 30\n",
      "Epoch 149 | T: 2.19 | Train RMSE: 3.03604 | Valid RMSE: 3.01531 | Current Roll Out: 30\n",
      "Epoch 150 | T: 2.19 | Train RMSE: 3.04369 | Valid RMSE: 2.98032 | Current Roll Out: 30\n",
      "Epoch 151 | T: 2.20 | Train RMSE: 3.04879 | Valid RMSE: 2.99801 | Current Roll Out: 30\n",
      "Epoch 152 | T: 2.19 | Train RMSE: 3.03761 | Valid RMSE: 3.08979 | Current Roll Out: 30\n",
      "Epoch 153 | T: 2.21 | Train RMSE: 3.03749 | Valid RMSE: 2.99419 | Current Roll Out: 30\n",
      "Epoch 154 | T: 2.20 | Train RMSE: 3.03325 | Valid RMSE: 3.02926 | Current Roll Out: 30\n",
      "Epoch 155 | T: 2.20 | Train RMSE: 3.03544 | Valid RMSE: 2.98073 | Current Roll Out: 30\n",
      "Epoch 156 | T: 2.20 | Train RMSE: 3.02609 | Valid RMSE: 3.04919 | Current Roll Out: 30\n",
      "Epoch 157 | T: 2.20 | Train RMSE: 3.04605 | Valid RMSE: 2.98690 | Current Roll Out: 30\n",
      "Epoch 158 | T: 2.19 | Train RMSE: 3.02947 | Valid RMSE: 3.00486 | Current Roll Out: 30\n",
      "Epoch 159 | T: 2.20 | Train RMSE: 3.02877 | Valid RMSE: 2.97435 | Current Roll Out: 30\n",
      "Epoch 160 | T: 2.19 | Train RMSE: 3.02055 | Valid RMSE: 2.99826 | Current Roll Out: 30\n",
      "Epoch 161 | T: 2.19 | Train RMSE: 3.02033 | Valid RMSE: 2.99363 | Current Roll Out: 30\n",
      "Epoch 162 | T: 2.19 | Train RMSE: 3.02888 | Valid RMSE: 2.98766 | Current Roll Out: 30\n",
      "Epoch 163 | T: 2.19 | Train RMSE: 3.01790 | Valid RMSE: 2.98401 | Current Roll Out: 30\n",
      "Epoch 164 | T: 2.19 | Train RMSE: 3.02387 | Valid RMSE: 2.98621 | Current Roll Out: 30\n",
      "Epoch 165 | T: 2.19 | Train RMSE: 3.02291 | Valid RMSE: 2.97695 | Current Roll Out: 30\n",
      "Epoch 166 | T: 2.19 | Train RMSE: 3.02298 | Valid RMSE: 3.08322 | Current Roll Out: 30\n",
      "Epoch 167 | T: 2.19 | Train RMSE: 3.02384 | Valid RMSE: 2.98055 | Current Roll Out: 30\n",
      "Epoch 168 | T: 2.19 | Train RMSE: 3.01984 | Valid RMSE: 2.98024 | Current Roll Out: 30\n",
      "Epoch 169 | T: 2.19 | Train RMSE: 3.01803 | Valid RMSE: 2.99768 | Current Roll Out: 30\n"
     ]
    }
   ],
   "source": [
    "train_rmse = []\n",
    "valid_rmse = []\n",
    "min_rmse = 10e8\n",
    "worse_count = 0\n",
    "\n",
    "roll_out_id = 0\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    start = time.time()\n",
    "\n",
    "    # model.train() # if you use dropout or batchnorm. \n",
    "    train_rmse.append(train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, loss_fun, roll_outs[roll_out_id]))\n",
    "\n",
    "    # model.eval()\n",
    "    val_rmse = eval_epoch(encoder, decoder, loss_fun, roll_outs[roll_out_id])\n",
    "    valid_rmse.append(val_rmse)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_rmse[-1] < min_rmse:\n",
    "        min_rmse = valid_rmse[-1]\n",
    "        best_model = (encoder, decoder)\n",
    "        worse_count = 0\n",
    "        # torch.save([best_model, i, get_lr(optimizer)], name + \".pth\")\n",
    "    else:\n",
    "        worse_count += 1\n",
    "    \n",
    "    if worse_count > 10:\n",
    "        roll_out_id += 1\n",
    "        worse_count = 0\n",
    "        min_rmse = 10e8\n",
    "        encoder, decoder = best_model\n",
    "    \n",
    "    if roll_out_id >= len(roll_outs):\n",
    "        break\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # Early Stopping\n",
    "    # if (len(train_rmse) > 100 and np.mean(valid_rmse[-5:]) >= np.mean(valid_rmse[-10:-5])):\n",
    "    #        break\n",
    "\n",
    "    # Learning Rate Decay        \n",
    "    encoder_scheduler.step()\n",
    "    decoder_scheduler.step()\n",
    "    \n",
    "    print(\"Epoch {} | T: {:0.2f} | Train RMSE: {:0.5f} | Valid RMSE: {:0.5f} | Current Roll Out: {}\".format(i + 1, (end-start) / 60, train_rmse[-1], valid_rmse[-1], roll_outs[roll_out_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d022f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3200/3200 [01:02<00:00, 51.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def inverse_transform_path(path, shift, rotation_matrix):\n",
    "    if path.ndim == 2:\n",
    "        return (np.linalg.inv(rotation_matrix) @ path.T).T + shift\n",
    "    elif path.ndim == 3:\n",
    "        path_normalize = np.zeros(path.shape)\n",
    "        for i in range(path.shape[0]):\n",
    "            path_normalize[i] = (np.linalg.inv(rotation_matrix) @ path[i].T).T + shift\n",
    "        return path_normalize\n",
    "    else:\n",
    "        raise Exception(\"Invalid dimension\")\n",
    "        \n",
    "encoder, decoder = best_model\n",
    "test_preds = []\n",
    "for i in tqdm(range(len(test_data[\"X\"]))):\n",
    "    inp = torch.from_numpy(test_data[\"X\"][i]).float().to(device).unsqueeze(0)\n",
    "    \n",
    "    embedded_vec = (\n",
    "        torch.zeros(1, embed_dim).to(device),\n",
    "        torch.zeros(1, embed_dim).to(device),\n",
    "    )\n",
    "    for step in range(19):\n",
    "        embedded_vec = encoder(inp[:, step, :], embedded_vec)\n",
    "\n",
    "    preds = []\n",
    "    pred = inp[:, step, :2]\n",
    "    for step in range(30):\n",
    "        pred, embedded_vec = decoder(pred, embedded_vec)\n",
    "        preds.append(pred.cpu().data.numpy())\n",
    "\n",
    "    preds = np.array(preds).reshape(30, 2)\n",
    "    \n",
    "    # De-Normalization !\n",
    "    preds = inverse_transform_path(preds, test_data[\"shifts\"][i], test_data[\"rotation_matrices\"][i])\n",
    "    test_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c127c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1).astype(int)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df.to_csv('test_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fced729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
